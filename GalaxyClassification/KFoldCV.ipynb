{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'TRAIN_PATH' : os.path.join('workspace', 'images','kfold', 'train'),\n",
    "    'TEST_PATH' : os.path.join('workspace', 'images','kfold','test'),\n",
    "    'IMAGES_PATH': os.path.join('workspace','images','all'),\n",
    "    'ANNOTATION_PATH': os.path.join('workspace','annotations'),\n",
    "    'RESULTS_PATH': os.path.join('workspace','results')\n",
    " }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_height = 69\n",
    "input_width = 69\n",
    "batch_size = 32\n",
    "\n",
    "augmentation=False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dynamic dataset rebalancing/folders creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function used to reset the dataset used in each different iteration\n",
    "def emptyFolders():\n",
    "    if not os.path.exists(os.path.join(paths[\"TRAIN_PATH\"])):\n",
    "        os.makedirs(os.path.join(paths[\"TRAIN_PATH\"]))\n",
    "    if not os.path.exists(os.path.join(paths[\"TEST_PATH\"])):\n",
    "        os.makedirs(os.path.join(paths[\"TEST_PATH\"]))\n",
    "    for i in range(0,10):\n",
    "        if not os.path.exists(os.path.join(paths[\"TRAIN_PATH\"],str(i))):\n",
    "            os.makedirs(os.path.join(paths[\"TRAIN_PATH\"],str(i)))\n",
    "        if not os.path.exists(os.path.join(paths[\"TEST_PATH\"],str(i))):\n",
    "            os.makedirs(os.path.join(paths[\"TEST_PATH\"],str(i)))\n",
    "\n",
    "        for file in os.listdir(os.path.join(paths[\"TRAIN_PATH\"],str(i))):\n",
    "            if file != \".DS_Store\":\n",
    "                os.remove(os.path.join(paths[\"TRAIN_PATH\"],str(i), file))\n",
    "        for file in os.listdir(os.path.join(paths[\"TEST_PATH\"],str(i))):\n",
    "            if file != \".DS_Store\":\n",
    "                os.remove(os.path.join(paths[\"TEST_PATH\"],str(i), file))\n",
    "\n",
    "df = pd.read_csv(os.path.join(paths['ANNOTATION_PATH'],\"annotations.csv\"))\n",
    "df = df.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import keras.utils as image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function used to dynamically delete elements in Class 5\n",
    "def remove_images(label):\n",
    "    img_names = os.listdir(os.path.join(paths['TRAIN_PATH'],str(label)))\n",
    "    for image in img_names:\n",
    "      f = os.path.join(paths['TRAIN_PATH'],str(label),image)\n",
    "      os.remove(f)\n",
    "\n",
    "# Function used to dinamically undersample the classes in the training set\n",
    "def undersample(label,n):\n",
    "    img_names = os.listdir(os.path.join(paths['TRAIN_PATH'],str(label)))\n",
    "    img_names = random.sample(img_names,n)  # Pick n random images to remove\n",
    "    for image in img_names:\n",
    "      f = os.path.join(paths['TRAIN_PATH'],str(label),image)\n",
    "      os.remove(f)\n",
    "\n",
    "# Function used to dinamically oversample the classes in the training set\n",
    "def augment_images(label,number_images, datagen):\n",
    "    number_images = int(number_images)\n",
    "    path = os.path.join(paths['TRAIN_PATH'],str(label))\n",
    "    i = 0\n",
    "    while i < number_images:\n",
    "        for f in os.listdir(path):\n",
    "          img = image.load_img(os.path.join(paths['TRAIN_PATH'],str(label),f), target_size=(69, 69))\n",
    "          x = image.img_to_array(img)\n",
    "          x = x.reshape((1,) + x.shape)\n",
    "          for batch in datagen.flow(x, batch_size=1):\n",
    "              new_image = image.array_to_img(batch[0])\n",
    "              new_image.save(os.path.join(paths['TRAIN_PATH'],str(label),\"aug_\" + str(i) +\".jpg\"))\n",
    "              i += 1\n",
    "              if i > number_images:\n",
    "                  return\n",
    "              break\n",
    "\n",
    "# Function used to dinamically remove folders' names after deleting Class 5\n",
    "def renameDirectories():\n",
    "    for i in range(6,10):\n",
    "        os.rename(os.path.join(paths['TRAIN_PATH'],str(i)),os.path.join(paths['TRAIN_PATH'],str(i-1)))\n",
    "        os.rename(os.path.join(paths['TEST_PATH'],str(i)),os.path.join(paths['TEST_PATH'],str(i-1)))\n",
    "\n",
    "# Function used to coordinate all the previous ones\n",
    "def rebalanceTrainingSet():\n",
    "    CLASS_TO_DELETE = 5\n",
    "    remove_images(CLASS_TO_DELETE)\n",
    "    N_IMG_TO_DELETE = [2017,1551]\n",
    "    CLASSES_TO_REDUCE = [1,2]\n",
    "    for i in range(len(CLASSES_TO_REDUCE)):\n",
    "        undersample(CLASSES_TO_REDUCE[i], N_IMG_TO_DELETE[i])\n",
    "    CLASSES_TO_AUGMENT = [3,4,6,7,8,9]\n",
    "    N_IMG_TO_AUGMENT = [600,400,600,500,500,600]\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "          rotation_range=40,\n",
    "          width_shift_range=0.2,\n",
    "          height_shift_range=0.1,\n",
    "          shear_range=0.2,\n",
    "          zoom_range=0.2,\n",
    "          horizontal_flip=True,\n",
    "          fill_mode='nearest')\n",
    "\n",
    "    for i in range(len(CLASSES_TO_AUGMENT)):\n",
    "        augment_images(CLASSES_TO_AUGMENT[i], N_IMG_TO_AUGMENT[i], datagen)\n",
    "    renameDirectories()\n",
    "\n",
    "# function used to plot the training distribution in order to check the balancing\n",
    "def plotTrainingDistribution():\n",
    "    files_per_label = dict()\n",
    "    for i in range(9):\n",
    "      path = os.path.join(paths['TRAIN_PATH'],str(i))\n",
    "      n_images = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "      files_per_label[i] = n_images\n",
    "    plt.bar(list(files_per_label.keys()), files_per_label.values(), color='g')\n",
    "    plt.show()\n",
    "    print(files_per_label)\n",
    "    return files_per_label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. K-Fold CV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import keras\n",
    "# Training with K-fold cross validation\n",
    "kf = StratifiedKFold(n_splits=K, random_state=None, shuffle=False)\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "for _, row in df.iterrows():\n",
    "    if(row['Path'].endswith(\".jpg\")):\n",
    "        new_row = { 'Path': row['Path'], 'Class': row['Class'] }\n",
    "        df_new_row = pd.DataFrame([new_row])\n",
    "        df_new = pd.concat([df_new, df_new_row],ignore_index=True)\n",
    "\n",
    "kf.get_n_splits(df_new)\n",
    "df_results = pd.DataFrame()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creation of the scratch model to be evaluated during K-Fold CV\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.optimizers as optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import keras.backend as KB\n",
    "\n",
    "# best loss function for multi-class classification, measures the distance between two probability distributions\n",
    "# the probability distribution of the output of the network and the true distribution of the labels\n",
    "loss_function='categorical_crossentropy'\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = KB.sum(KB.round(KB.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = KB.sum(KB.round(KB.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + KB.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = KB.sum(KB.round(KB.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = KB.sum(KB.round(KB.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + KB.epsilon())\n",
    "    return recall\n",
    "\n",
    "metrics = [\n",
    "    precision,\n",
    "    recall,\n",
    "    tf.keras.metrics.CategoricalAccuracy(name='acc')\n",
    "]\n",
    "optimizer='rmsprop'\n",
    "optimizer_learning_rate=1e-4\n",
    "epochs=100\n",
    "batch_size=32\n",
    "regularizer=regularizers.l1_l2(l1=0.001, l2=0.001) # simultaneous l1 and l2, add 0.001*weight_coefficient_value + 0.001 * 1/2*weight^2\n",
    "\n",
    "if optimizer == 'rmsprop':\n",
    "    optimizer=optimizers.RMSprop(learning_rate=optimizer_learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_scratch_CNN(width, height, depth, num_classes,filters=(16, 32, 64)):\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "    model = models.Sequential()\n",
    "    for (i, f) in enumerate(filters):\n",
    "        if i == 0:\n",
    "            model.add(Conv2D(f, (3, 3), activation=\"relu\", padding=\"same\", input_shape=inputShape))\n",
    "        else:\n",
    "            model.add(Conv2D(f, (3, 3), activation=\"relu\", padding=\"same\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # FLATTEN => FC => RELU => BN => DROPOUT\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation=\"relu\")) # consider if we need to add this dense layer before with more units, such as 64 in order to shrink in two different stages, depends on the outpout size of flatten\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "model = create_scratch_CNN(input_width, input_height, 3, 9, (16,32,64,128))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "def create_finetuned_VGG16():\n",
    "    conv_base = VGG16(weights='imagenet',\n",
    "                                include_top=False, # exclude fully connected layer\n",
    "                                input_shape=(input_width, input_height, 3))\n",
    "    conv_base.trainable = True\n",
    "    set_trainable = False\n",
    "    for layer in conv_base.layers:\n",
    "        if layer.name.startswith(\"block5\"): # fine tune layers from block5_*\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "    built_model = models.Sequential()\n",
    "    built_model.add(conv_base)\n",
    "    # add fully connected layer\n",
    "    built_model.add(Flatten())\n",
    "    built_model.add(Dense(64, activation='relu'))\n",
    "    built_model.add(Dense(9, activation='softmax'))\n",
    "    return built_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()\n",
    "print(df_results)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(df_new, df_new['Class']):\n",
    "    print(\"====== K-Fold CV Iteration: %d/%d =======\" % (i,K))\n",
    "    emptyFolders()\n",
    "    print(\"Folders empty..\")\n",
    "    print(\"Train: \", len(train_index))\n",
    "    print(\"Test: \", len(test_index))\n",
    "    df_train = df_new.iloc[train_index]\n",
    "    df_test = df_new.iloc[test_index]\n",
    "    # dinamically create the folders containing the fold's images\n",
    "    for _, row in df_train.iterrows():\n",
    "        try:\n",
    "            shutil.copy(os.path.join(paths[\"IMAGES_PATH\"],row['Path']),\n",
    "                    os.path.join(paths[\"TRAIN_PATH\"],str(row['Class']),row['Path']))\n",
    "        except:\n",
    "            pass\n",
    "    for _, row in df_test.iterrows():\n",
    "        try:\n",
    "            shutil.copy(os.path.join(paths[\"IMAGES_PATH\"],row['Path']),\n",
    "                    os.path.join(paths[\"TEST_PATH\"],str(row['Class']),row['Path']))\n",
    "        except:\n",
    "            pass\n",
    "    print(\"Images moved...\")\n",
    "    # rebalancing\n",
    "    rebalanceTrainingSet()\n",
    "    files_per_label = plotTrainingDistribution()\n",
    "\n",
    "    # training set image data generator\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    train_dir=paths['TRAIN_PATH']\n",
    "    train_generator = train_datagen.flow_from_directory(train_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "    # validation set image data generator\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    validation_dir=paths['TEST_PATH']\n",
    "    validation_generator = val_datagen.flow_from_directory(validation_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "    VGG16model = create_finetuned_VGG16()\n",
    "    VGG16model.compile(optimizer=optimizer,\n",
    "                  loss=loss_function,\n",
    "                  metrics=metrics)\n",
    "\n",
    "    scratchModel = create_scratch_CNN(input_width, input_height, 3, 9, (16,32,64,128))\n",
    "    scratchModel.compile(optimizer=optimizer,\n",
    "                  loss=loss_function,\n",
    "                  metrics=metrics)\n",
    "\n",
    "    num_classes = 9\n",
    "    tot_images = sum(list(files_per_label.values()))\n",
    "    weights = dict([ (class_label , tot_images/(num_classes * n_images)) for class_label, n_images in files_per_label.items()])\n",
    "\n",
    "    n_images_eval = 0\n",
    "    for j in range(9):\n",
    "        path = os.path.join(paths['TEST_PATH'],str(j))\n",
    "        n_images_eval = n_images_eval + len([f for f in os.listdir(path)if os.path.isfile(os.path.join(path, f))])\n",
    "\n",
    "    number_training = tot_images\n",
    "    number_eval = n_images_eval\n",
    "\n",
    "    callbacks_list = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', # should be part of the metrics specific during compilation\n",
    "            patience=2\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2, # divides LR by 5 when triggered\n",
    "            patience=3 # called when stopped improving for 3 epochs\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    #epochs=20\n",
    "\n",
    "    history = scratchModel.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=int(math.ceil((1. * number_training) / batch_size)),\n",
    "      epochs=epochs,\n",
    "      validation_data=validation_generator,\n",
    "      callbacks=callbacks_list,\n",
    "      validation_steps=int(math.ceil((1. * number_eval) / batch_size)))\n",
    "\n",
    "    acc = history.history['acc'][len(history.history['acc'])-1]\n",
    "    val_acc = history.history['val_acc'][len(history.history['acc'])-1]\n",
    "    val_recall = history.history['val_recall'][len(history.history['acc'])-1]\n",
    "    val_precision = history.history['val_precision'][len(history.history['acc'])-1]\n",
    "    new_row = { 'model': 'scratch', 'k': i+1, 'epochs': epochs, 'acc': acc, 'test_acc': val_acc, 'test_recall': val_recall, 'test_precision': val_precision}\n",
    "    df_new_row = pd.DataFrame([new_row])\n",
    "    df_results = pd.concat([df_results, df_new_row],ignore_index=True)\n",
    "\n",
    "    history = VGG16model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=int(math.ceil((1. * number_training) / batch_size)),\n",
    "      epochs=epochs,\n",
    "      validation_data=validation_generator,\n",
    "      callbacks=callbacks_list,\n",
    "      validation_steps=int(math.ceil((1. * number_eval) / batch_size)))\n",
    "\n",
    "    acc = history.history['acc'][len(history.history['acc'])-1]\n",
    "    val_acc = history.history['val_acc'][len(history.history['acc'])-1]\n",
    "    val_recall = history.history['val_recall'][len(history.history['acc'])-1]\n",
    "    val_precision = history.history['val_precision'][len(history.history['acc'])-1]\n",
    "    new_row = { 'model': 'VGG16', 'k': i+1, 'epochs': epochs, 'acc': acc, 'test_acc': val_acc, 'test_recall': val_recall, 'test_precision': val_precision}\n",
    "    df_new_row = pd.DataFrame([new_row])\n",
    "    df_results = pd.concat([df_results, df_new_row],ignore_index=True)\n",
    "\n",
    "    i+=1\n",
    "    print(df_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_results.to_csv(os.path.join(paths['RESULTS_PATH'], \"resultsKFold.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
