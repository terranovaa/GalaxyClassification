{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPwwbJDjw-Bv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from keras_tuner.applications import HyperResNet\n",
    "from keras_tuner.applications import HyperXception\n",
    "from keras_tuner import  HyperParameters\n",
    "import tensorflow as tf\n",
    "import matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9iB5X1u1nGM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cTFqgkG1aLd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " # read dataset\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "train_images = train_images.reshape((n_train, 256, 256, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((n_test, 256, 256, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "#to categorical (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#early stopping callbacks, used for each model\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tensorboard = keras.callbacks.TensorBoard(\"log_dir\")\n",
    "\n",
    "NUM_CLASSES = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#tune a model given an hypermodel, find best hyperparameters, and return it\n",
    "def tune_model(hypermodel):\n",
    "\n",
    "    tuner = keras_tuner.Hyperband( #decide objective and type of tuner\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    directory='galaxy_dir',\n",
    "    project_name='galaxy_classification_project'\n",
    "\n",
    "    )\n",
    "\n",
    "    #find best parameters\n",
    "    tuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early, tensorboard])\n",
    "\n",
    "    # Get the optimal hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    #tune model with optimal parameters\n",
    "    tuned_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    return tuned_model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJv7UIuf1rGi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ihF_Qld1aae",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CNNFromScratch(keras_tuner.HyperModel):\n",
    "    #override build\n",
    "    def build_model(self,hp):\n",
    "      model = models.Sequential()\n",
    "\n",
    "      #add input layer\n",
    "      model.add(layers.Input(shape=(256,256,1)))\n",
    "\n",
    "      #add data augmentation layer???\n",
    "\n",
    "      # Tune the number of layers.\n",
    "      for i in range(hp.Int(\"num_layers\", min_value = 1, max_value = 64, step = 8)):\n",
    "\n",
    "          #tune filter size\n",
    "          hp_filters = hp.Int(f\"conv_filter_{i}\", min_value=16, max_value=64, step=16)\n",
    "          #tune filter size. Most used are 3x3 and 5x5\n",
    "          hp_kernel_size=hp.Choice(\"conv_kernel\", values = [3,5])\n",
    "          #add convolutional and pooling layer\n",
    "          model.add(layers.Conv2D(filters = hp_filters, kernel_size = (hp_kernel_size,hp_kernel_size), activation='relu'))\n",
    "          model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "          model.add(layers.BatchNormalization())#da tunare?\n",
    "\n",
    "      # flatten ed eventuale dense prima (tunare presenza)\n",
    "      model.add(layers.Flatten())\n",
    "      if hp.Boolean(\"dense_layer\"):\n",
    "          model.add(layers.Dense())\n",
    "\n",
    "      #decide whether to use dropout or not\n",
    "      if hp.Boolean(\"dropout\"):\n",
    "          hp_dropout_rate = hp.Int(f\"dropout_rate\", min_value=0.5, max_value=0.8, step=0.1)\n",
    "          model.add(layers.Dropout(rate=0.25))\n",
    "\n",
    "      #add classification layer\n",
    "      model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "      #tune learning rate\n",
    "      hp_learning_rate = hp.Float(\"learning rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "      model.compile(\n",
    "          optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate), # tuning optimizer\n",
    "          loss=\"categorical_crossentropy\",\n",
    "          metrics=[\"accuracy\"],\n",
    "      )\n",
    "\n",
    "      return model\n",
    "\n",
    "    #override fit\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            epochs=hp.Int(\"epochs\", min_value = 5, max_value = 20, step = 5),\n",
    "            batch_size=hp.Choice(\"batch_size\", [16, 32]),\n",
    "            **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#get tuned models\n",
    "model_cnn = tune_model(CNNFromScratch())\n",
    "model_resnet = tune_model(HyperResNet(input_shape=(256, 256, 1), classes=NUM_CLASSES))\n",
    "model_xception = tune_model(HyperXception(input_shape=(256, 256, 1), classes=NUM_CLASSES))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZK41Nu971umZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Can delete from now on I guess(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC0Ap5Oi18YU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuner = keras_tuner.Hyperband( #decide objective and type of tuner\n",
    "    CNNFromScratch(),\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    directory='galaxy_dir',\n",
    "    project_name='galaxy_classification_project'\n",
    "\n",
    ")\n",
    "\n",
    "#find best parameters\n",
    "tuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early, tensorboard])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "#tune model with optimal parameters\n",
    "tuned_model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41yfK-QjGACI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Possibile test con entrambi gli approcci ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-mmetN-FN9H",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuner = keras_tuner.Hyperband(\n",
    "    HyperResNet(input_shape=(256, 256, 1), classes=10), #used a in-built model for tuning, useful in CV applications.\n",
    "                                                      #there is no need to define a search space so users can use it directly\n",
    "    objective='val_accuracy', #decide objective\n",
    "    max_epochs=10,\n",
    "    directory='galaxy_dir',  #store search result in this dir\n",
    "    project_name='galaxy_classification_project' # will be a sub dir of the above directory\n",
    ")\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir)\n",
    "tuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early, tensorboard])\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "tuned_model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqdQHzIhciF7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuner = keras_tuner.Hyperband(\n",
    "    HyperXception(input_shape=(256, 256, 1), classes=10), #used a in-built model for tuning, useful in CV applications.\n",
    "                                                      #there is no need to define a search space so users can use it directly\n",
    "    objective='val_accuracy', #decide objective\n",
    "    max_epochs=10,\n",
    "    directory='galaxy_dir',  #store search result in this dir\n",
    "    project_name='galaxy_classification_project' # will be a sub dir of the above directory\n",
    ")\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir)\n",
    "tuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early, tensorboard])\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "tuned_model = tuner.hypermodel.build(best_hps)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}