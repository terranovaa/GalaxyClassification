{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\r\n",
      "  Downloading matplotlib-3.6.2-cp310-cp310-macosx_10_12_x86_64.whl (7.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.3/7.3 MB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting pillow>=6.2.0\r\n",
      "  Downloading Pillow-9.3.0-cp310-cp310-macosx_10_10_x86_64.whl (3.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m7.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting fonttools>=4.22.0\r\n",
      "  Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/GalaxyClassification/lib/python3.10/site-packages (from matplotlib) (21.3)\r\n",
      "Collecting kiwisolver>=1.0.1\r\n",
      "  Using cached kiwisolver-1.4.4-cp310-cp310-macosx_10_9_x86_64.whl (65 kB)\r\n",
      "Requirement already satisfied: numpy>=1.19 in /opt/anaconda3/envs/GalaxyClassification/lib/python3.10/site-packages (from matplotlib) (1.23.4)\r\n",
      "Collecting contourpy>=1.0.1\r\n",
      "  Downloading contourpy-1.0.6-cp310-cp310-macosx_10_9_x86_64.whl (240 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m240.6/240.6 kB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting cycler>=0.10\r\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/anaconda3/envs/GalaxyClassification/lib/python3.10/site-packages (from matplotlib) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/GalaxyClassification/lib/python3.10/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/GalaxyClassification/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\r\n",
      "Successfully installed contourpy-1.0.6 cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.6.2 pillow-9.3.0\r\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpip install matplotlib\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptimizers\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01moptimizers\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ImageDataGenerator\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.optimizers as optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "input_height = 69\n",
    "input_width = 69\n",
    "batch_size = 64\n",
    "\n",
    "# TODO: find best parameters using the display_data_augmentation_sample jupyter notebook\n",
    "rescale = True\n",
    "if rescale:\n",
    "    rescale_size=1./255\n",
    "else:\n",
    "    rescale_size=1\n",
    "augmentation=True\n",
    "\n",
    "rotation_range=40\n",
    "width_shift_range=0.2\n",
    "height_shift_range=0.2\n",
    "shear_range=0.2\n",
    "zoom_range=0.2\n",
    "horizontal_flip=True\n",
    "fill_mode='nearest'\n",
    "# consider also stride (>1), type of pooling (Max, Avg), padding (same, valid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# best loss function for multi-class classification, measures the distance between two probability distributions\n",
    "# the probability distribution of the output of the network and the true distribution of the labels\n",
    "loss_function='categorical_crossentropy'\n",
    "\n",
    "metrics=['accuracy']\n",
    "optimizer='rmsprop'\n",
    "optimizer_learning_rate=1e-4\n",
    "epochs=300\n",
    "batch_size=20\n",
    "regularizer=regularizers.l1_l2(l1=0.001, l2=0.001) # simultaneous l1 and l2, add 0.001*weight_coefficient_value + 0.001 * 1/2*weight^2\n",
    "\n",
    "if optimizer == 'rmsprop':\n",
    "    optimizer=optimizers.RMSprop(learning_rate=optimizer_learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Holdout method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'TRAIN_PATH' : os.path.join('workspace', 'images', 'train'),\n",
    "    'TEST_PATH' : os.path.join('workspace', 'images','test'),\n",
    "    'EVAL_PATH' : os.path.join('workspace', 'images','eval'),\n",
    "    'IMAGES_PATH': os.path.join('workspace','images','all'),\n",
    "    'ANNOTATION_PATH': os.path.join('workspace','annotations'),\n",
    "    'LOG_DIR' : os.path.join('model', 'log_dir')\n",
    " }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# create paths\n",
    "for path in paths.values():\n",
    "    if not os.path.exists(path):\n",
    "        if os.name == 'posix':\n",
    "            !mkdir -p {path}\n",
    "        if os.name == 'nt':\n",
    "            !mkdir {path}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 19606\n",
      "Test size: 2179\n",
      "Eval size: 3922\n"
     ]
    }
   ],
   "source": [
    "# remove all files already present for an old holdout method execution\n",
    "for i in range(0,10):\n",
    "    for file in os.listdir(os.path.join(paths[\"TRAIN_PATH\"],str(i))):\n",
    "        if file != \".DS_Store\":\n",
    "            os.remove(os.path.join(paths[\"TRAIN_PATH\"],str(i), file))\n",
    "    for file in os.listdir(os.path.join(paths[\"TEST_PATH\"],str(i))):\n",
    "        if file != \".DS_Store\":\n",
    "            os.remove(os.path.join(paths[\"TEST_PATH\"],str(i), file))\n",
    "    for file in os.listdir(os.path.join(paths[\"EVAL_PATH\"],str(i))):\n",
    "        if file != \".DS_Store\":\n",
    "            os.remove(os.path.join(paths[\"EVAL_PATH\"],str(i), file))\n",
    "\n",
    "df = pd.read_csv(os.path.join(paths['ANNOTATION_PATH'],\"annotations.csv\"))\n",
    "df = df.dropna()\n",
    "\n",
    "for i in range(0,10):\n",
    "    if not os.path.exists(os.path.join(paths[\"EVAL_PATH\"],str(i))):\n",
    "        if os.name == 'posix':\n",
    "            !mkdir -p {os.path.join(paths[\"EVAL_PATH\"],str(i))}\n",
    "    if not os.path.exists(os.path.join(paths[\"TRAIN_PATH\"],str(i))):\n",
    "        if os.name == 'posix':\n",
    "            !mkdir -p {os.path.join(paths[\"TRAIN_PATH\"],str(i))}\n",
    "    if not os.path.exists(os.path.join(paths[\"TEST_PATH\"],str(i))):\n",
    "        if os.name == 'posix':\n",
    "            !mkdir -p {os.path.join(paths[\"TEST_PATH\"],str(i))}\n",
    "\n",
    "# split in training set (90%) and test set (10%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df, test_size=0.1, shuffle=True)\n",
    "print(\"Train size: \" + str(len(X_train)))\n",
    "\n",
    "for index, row in X_test.iterrows():\n",
    "    shutil.copy(os.path.join(paths[\"IMAGES_PATH\"],str(row['Path'])),\n",
    "                os.path.join(paths[\"TEST_PATH\"],str(row['Class']),str(row['Path'])))\n",
    "\n",
    "# split the training set into the final training set (80%) and validation set (20%)\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_train, y_train, test_size=0.2, shuffle=True)\n",
    "print(\"Test size: \" + str(len(X_test)))\n",
    "print(\"Eval size: \" + str(len(X_eval)))\n",
    "\n",
    "for index, row in X_train.iterrows():\n",
    "    shutil.copy(os.path.join(paths[\"IMAGES_PATH\"],str(row['Path'])),\n",
    "                os.path.join(paths[\"TRAIN_PATH\"],str(row['Class']),str(row['Path'])))\n",
    "\n",
    "for _, row in X_eval.iterrows():\n",
    "    shutil.copy(os.path.join(paths[\"IMAGES_PATH\"],str(row['Path'])),\n",
    "                os.path.join(paths[\"EVAL_PATH\"],str(row['Class']),str(row['Path'])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15684 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# training set image data generator\n",
    "if augmentation:\n",
    "    train_datagen = ImageDataGenerator(\n",
    "          rescale=rescale_size,\n",
    "          rotation_range=rotation_range,\n",
    "          width_shift_range=width_shift_range,\n",
    "          height_shift_range=height_shift_range,\n",
    "          shear_range=shear_range,\n",
    "          zoom_range=zoom_range,\n",
    "          horizontal_flip=horizontal_flip,\n",
    "          fill_mode=fill_mode)\n",
    "else:\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    # to perform normalization we should never use information coming from the test set, only training set\n",
    "\n",
    "train_dir=paths['TRAIN_PATH']\n",
    "\n",
    "# TODO: Consider if the output should be normalized\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3922 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# validation set image data generator\n",
    "val_datagen = ImageDataGenerator(rescale=rescale_size) # it should not be augmented\n",
    "\n",
    "validation_dir=paths['EVAL_PATH']\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(validation_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Model creation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 09:45:39.170266: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 69, 69, 8)         224       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 69, 69, 8)        32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 34, 34, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 34, 34, 16)        1168      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 34, 34, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 17, 17, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4624)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4624)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                296000    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 298,394\n",
      "Trainable params: 298,218\n",
      "Non-trainable params: 176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Function used to create the CNN structure used for regression\n",
    "def create_cnn(width, height, depth, num_classes,filters=(16, 32, 64)):\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "    model = models.Sequential()\n",
    "    for (i, f) in enumerate(filters):\n",
    "        # first CONV layer set appropriately\n",
    "        if i == 0:\n",
    "            model.add(Conv2D(f, (3, 3), activation=\"relu\", padding=\"same\", kernel_regularizer=regularizer, input_shape=inputShape))\n",
    "        else:\n",
    "            model.add(Conv2D(f, (3, 3), activation=\"relu\", padding=\"same\", kernel_regularizer=regularizer))\n",
    "        # size of the patches typically 3x3 or 5x5\n",
    "        # determine if we need to change padding or stride, with padding = same we are able to center convolutional windows around every input tile, in order to have always the same size of the input image\n",
    "        # model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # FLATTEN => FC => RELU => BN => DROPOUT\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation=\"relu\")) # consider if we need to add this dense layer before with more units, such as 64 in order to shrink in two different stages, depends on the outpout size of flatten\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "model = create_cnn(input_width, input_height, 3, 10, (8,16))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_function,\n",
    "              metrics=metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Model training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "785/785 [==============================] - 87s 110ms/step - loss: 1.8422 - accuracy: 0.3947 - val_loss: 1.6217 - val_accuracy: 0.4653\n",
      "Epoch 2/300\n",
      "785/785 [==============================] - 88s 112ms/step - loss: 1.6484 - accuracy: 0.4316 - val_loss: 1.5210 - val_accuracy: 0.4605\n",
      "Epoch 3/300\n",
      "785/785 [==============================] - 97s 124ms/step - loss: 1.5477 - accuracy: 0.4507 - val_loss: 1.3981 - val_accuracy: 0.4921\n",
      "Epoch 4/300\n",
      "785/785 [==============================] - 84s 106ms/step - loss: 1.4888 - accuracy: 0.4574 - val_loss: 1.3604 - val_accuracy: 0.4980\n",
      "Epoch 5/300\n",
      "785/785 [==============================] - 79s 101ms/step - loss: 1.4337 - accuracy: 0.4742 - val_loss: 1.3221 - val_accuracy: 0.5209\n",
      "Epoch 6/300\n",
      "785/785 [==============================] - 107s 137ms/step - loss: 1.4068 - accuracy: 0.4762 - val_loss: 1.3275 - val_accuracy: 0.5107\n",
      "Epoch 7/300\n",
      "785/785 [==============================] - 85s 109ms/step - loss: 1.3854 - accuracy: 0.4892 - val_loss: 1.2583 - val_accuracy: 0.5495\n",
      "Epoch 8/300\n",
      "785/785 [==============================] - 86s 110ms/step - loss: 1.3649 - accuracy: 0.4902 - val_loss: 1.2763 - val_accuracy: 0.5235\n",
      "Epoch 9/300\n",
      "785/785 [==============================] - 88s 112ms/step - loss: 1.3481 - accuracy: 0.5008 - val_loss: 1.2272 - val_accuracy: 0.5592\n",
      "Epoch 10/300\n",
      "785/785 [==============================] - 70s 90ms/step - loss: 1.3388 - accuracy: 0.4971 - val_loss: 1.2059 - val_accuracy: 0.5548\n",
      "Epoch 11/300\n",
      "785/785 [==============================] - 61s 77ms/step - loss: 1.3239 - accuracy: 0.5088 - val_loss: 1.2398 - val_accuracy: 0.5291\n",
      "Epoch 12/300\n",
      "785/785 [==============================] - 77s 98ms/step - loss: 1.3093 - accuracy: 0.5149 - val_loss: 1.2478 - val_accuracy: 0.5298\n",
      "Epoch 13/300\n",
      "785/785 [==============================] - 75s 95ms/step - loss: 1.2957 - accuracy: 0.5202 - val_loss: 1.1673 - val_accuracy: 0.5964\n",
      "Epoch 14/300\n",
      "785/785 [==============================] - 57s 73ms/step - loss: 1.2888 - accuracy: 0.5212 - val_loss: 1.2595 - val_accuracy: 0.5112\n",
      "Epoch 15/300\n",
      "785/785 [==============================] - 68s 86ms/step - loss: 1.2828 - accuracy: 0.5190 - val_loss: 1.1601 - val_accuracy: 0.5755\n",
      "Epoch 16/300\n",
      "785/785 [==============================] - 86s 110ms/step - loss: 1.2646 - accuracy: 0.5339 - val_loss: 1.1588 - val_accuracy: 0.5834\n",
      "Epoch 17/300\n",
      "785/785 [==============================] - 75s 95ms/step - loss: 1.2540 - accuracy: 0.5338 - val_loss: 1.1507 - val_accuracy: 0.5811\n",
      "Epoch 18/300\n",
      "785/785 [==============================] - 74s 94ms/step - loss: 1.2608 - accuracy: 0.5301 - val_loss: 1.1368 - val_accuracy: 0.5760\n",
      "Epoch 19/300\n",
      "785/785 [==============================] - 70s 89ms/step - loss: 1.2483 - accuracy: 0.5392 - val_loss: 1.1299 - val_accuracy: 0.5811\n",
      "Epoch 20/300\n",
      "785/785 [==============================] - 62s 79ms/step - loss: 1.2432 - accuracy: 0.5370 - val_loss: 1.1385 - val_accuracy: 0.6002\n",
      "Epoch 21/300\n",
      "785/785 [==============================] - 60s 77ms/step - loss: 1.2323 - accuracy: 0.5493 - val_loss: 1.1412 - val_accuracy: 0.5775\n",
      "Epoch 22/300\n",
      "785/785 [==============================] - 60s 77ms/step - loss: 1.2318 - accuracy: 0.5441 - val_loss: 1.1085 - val_accuracy: 0.5844\n",
      "Epoch 23/300\n",
      "785/785 [==============================] - 65s 83ms/step - loss: 1.2187 - accuracy: 0.5485 - val_loss: 1.1316 - val_accuracy: 0.5900\n",
      "Epoch 24/300\n",
      "785/785 [==============================] - 64s 82ms/step - loss: 1.2170 - accuracy: 0.5506 - val_loss: 1.0784 - val_accuracy: 0.6091\n",
      "Epoch 25/300\n",
      "785/785 [==============================] - 62s 79ms/step - loss: 1.2059 - accuracy: 0.5502 - val_loss: 1.1077 - val_accuracy: 0.5821\n",
      "Epoch 26/300\n",
      "785/785 [==============================] - 71s 90ms/step - loss: 1.2011 - accuracy: 0.5572 - val_loss: 1.0553 - val_accuracy: 0.6257\n",
      "Epoch 27/300\n",
      "785/785 [==============================] - 62s 79ms/step - loss: 1.1975 - accuracy: 0.5622 - val_loss: 1.0448 - val_accuracy: 0.6364\n",
      "Epoch 28/300\n",
      "785/785 [==============================] - 61s 78ms/step - loss: 1.1846 - accuracy: 0.5620 - val_loss: 1.0956 - val_accuracy: 0.5765\n",
      "Epoch 29/300\n",
      "785/785 [==============================] - 67s 85ms/step - loss: 1.1875 - accuracy: 0.5640 - val_loss: 1.0649 - val_accuracy: 0.6061\n",
      "Epoch 30/300\n",
      "785/785 [==============================] - 68s 86ms/step - loss: 1.1760 - accuracy: 0.5640 - val_loss: 1.0570 - val_accuracy: 0.6020\n",
      "Epoch 31/300\n",
      "785/785 [==============================] - 72s 91ms/step - loss: 1.1663 - accuracy: 0.5718 - val_loss: 1.0246 - val_accuracy: 0.6464\n",
      "Epoch 32/300\n",
      "785/785 [==============================] - 66s 84ms/step - loss: 1.1632 - accuracy: 0.5692 - val_loss: 1.0284 - val_accuracy: 0.6494\n",
      "Epoch 33/300\n",
      "785/785 [==============================] - 71s 91ms/step - loss: 1.1494 - accuracy: 0.5789 - val_loss: 1.0620 - val_accuracy: 0.6415\n",
      "Epoch 34/300\n",
      "785/785 [==============================] - 71s 90ms/step - loss: 1.1496 - accuracy: 0.5816 - val_loss: 1.0181 - val_accuracy: 0.6318\n",
      "Epoch 35/300\n",
      "785/785 [==============================] - 67s 86ms/step - loss: 1.1498 - accuracy: 0.5704 - val_loss: 1.0127 - val_accuracy: 0.6382\n",
      "Epoch 36/300\n",
      "785/785 [==============================] - 67s 85ms/step - loss: 1.1445 - accuracy: 0.5787 - val_loss: 1.0066 - val_accuracy: 0.6458\n",
      "Epoch 37/300\n",
      "785/785 [==============================] - 68s 87ms/step - loss: 1.1386 - accuracy: 0.5809 - val_loss: 1.0555 - val_accuracy: 0.6091\n",
      "Epoch 38/300\n",
      "785/785 [==============================] - 67s 85ms/step - loss: 1.1281 - accuracy: 0.5838 - val_loss: 1.0165 - val_accuracy: 0.6168\n",
      "Epoch 39/300\n",
      "785/785 [==============================] - 67s 85ms/step - loss: 1.1297 - accuracy: 0.5923 - val_loss: 1.0673 - val_accuracy: 0.6242\n",
      "Epoch 40/300\n",
      "785/785 [==============================] - 67s 85ms/step - loss: 1.1234 - accuracy: 0.5918 - val_loss: 1.0000 - val_accuracy: 0.6344\n",
      "Epoch 41/300\n",
      "785/785 [==============================] - 67s 85ms/step - loss: 1.1199 - accuracy: 0.5928 - val_loss: 1.0333 - val_accuracy: 0.6043\n",
      "Epoch 42/300\n",
      "785/785 [==============================] - 67s 85ms/step - loss: 1.1108 - accuracy: 0.5961 - val_loss: 0.9922 - val_accuracy: 0.6512\n",
      "Epoch 43/300\n",
      "785/785 [==============================] - 69s 88ms/step - loss: 1.1154 - accuracy: 0.5909 - val_loss: 0.9872 - val_accuracy: 0.6359\n",
      "Epoch 44/300\n",
      "785/785 [==============================] - 68s 87ms/step - loss: 1.1087 - accuracy: 0.5950 - val_loss: 0.9911 - val_accuracy: 0.6356\n",
      "Epoch 45/300\n",
      "785/785 [==============================] - 68s 86ms/step - loss: 1.1044 - accuracy: 0.5969 - val_loss: 1.0025 - val_accuracy: 0.6254\n",
      "Epoch 46/300\n",
      "785/785 [==============================] - 68s 86ms/step - loss: 1.1052 - accuracy: 0.5937 - val_loss: 0.9882 - val_accuracy: 0.6331\n",
      "Epoch 47/300\n",
      "785/785 [==============================] - 67s 86ms/step - loss: 1.1027 - accuracy: 0.5935 - val_loss: 1.0104 - val_accuracy: 0.6247\n",
      "Epoch 48/300\n",
      "785/785 [==============================] - 67s 85ms/step - loss: 1.0945 - accuracy: 0.5990 - val_loss: 0.9976 - val_accuracy: 0.6543\n",
      "Epoch 49/300\n",
      "785/785 [==============================] - 67s 86ms/step - loss: 1.1022 - accuracy: 0.5991 - val_loss: 0.9345 - val_accuracy: 0.6754\n",
      "Epoch 50/300\n",
      "785/785 [==============================] - 68s 87ms/step - loss: 1.0979 - accuracy: 0.6020 - val_loss: 0.9951 - val_accuracy: 0.6239\n",
      "Epoch 51/300\n",
      "785/785 [==============================] - 67s 85ms/step - loss: 1.0858 - accuracy: 0.6032 - val_loss: 0.9895 - val_accuracy: 0.6356\n",
      "Epoch 52/300\n",
      "785/785 [==============================] - 67s 85ms/step - loss: 1.0920 - accuracy: 0.6025 - val_loss: 0.9979 - val_accuracy: 0.6372\n",
      "Epoch 53/300\n",
      "785/785 [==============================] - 67s 85ms/step - loss: 1.0838 - accuracy: 0.6053 - val_loss: 0.9953 - val_accuracy: 0.6339\n",
      "Epoch 54/300\n",
      "785/785 [==============================] - 70s 89ms/step - loss: 1.0926 - accuracy: 0.6004 - val_loss: 0.9493 - val_accuracy: 0.6808\n",
      "Epoch 55/300\n",
      "785/785 [==============================] - 76s 97ms/step - loss: 1.0811 - accuracy: 0.6081 - val_loss: 0.9728 - val_accuracy: 0.6474\n",
      "Epoch 56/300\n",
      "785/785 [==============================] - 64s 82ms/step - loss: 1.0838 - accuracy: 0.6054 - val_loss: 0.9692 - val_accuracy: 0.6553\n",
      "Epoch 57/300\n",
      "785/785 [==============================] - 64s 82ms/step - loss: 1.0841 - accuracy: 0.6023 - val_loss: 1.0139 - val_accuracy: 0.6163\n",
      "Epoch 58/300\n",
      "785/785 [==============================] - 64s 81ms/step - loss: 1.0747 - accuracy: 0.6079 - val_loss: 0.9151 - val_accuracy: 0.6953\n",
      "Epoch 59/300\n",
      "785/785 [==============================] - 64s 81ms/step - loss: 1.0706 - accuracy: 0.6129 - val_loss: 1.0096 - val_accuracy: 0.6328\n",
      "Epoch 60/300\n",
      "785/785 [==============================] - 68s 86ms/step - loss: 1.0707 - accuracy: 0.6118 - val_loss: 0.9225 - val_accuracy: 0.6815\n",
      "Epoch 61/300\n",
      "785/785 [==============================] - 63s 81ms/step - loss: 1.0728 - accuracy: 0.6108 - val_loss: 1.0356 - val_accuracy: 0.6267\n",
      "Epoch 62/300\n",
      "785/785 [==============================] - 65s 83ms/step - loss: 1.0712 - accuracy: 0.6071 - val_loss: 0.9438 - val_accuracy: 0.6660\n",
      "Epoch 63/300\n",
      "785/785 [==============================] - 65s 82ms/step - loss: 1.0670 - accuracy: 0.6137 - val_loss: 0.9961 - val_accuracy: 0.6318\n",
      "Epoch 64/300\n",
      "785/785 [==============================] - 62s 78ms/step - loss: 1.0710 - accuracy: 0.6123 - val_loss: 0.9313 - val_accuracy: 0.6634\n",
      "Epoch 65/300\n",
      "785/785 [==============================] - 68s 86ms/step - loss: 1.0676 - accuracy: 0.6134 - val_loss: 0.9845 - val_accuracy: 0.6369\n",
      "Epoch 66/300\n",
      "785/785 [==============================] - 63s 81ms/step - loss: 1.0608 - accuracy: 0.6177 - val_loss: 1.0770 - val_accuracy: 0.6130\n",
      "Epoch 67/300\n",
      "785/785 [==============================] - 64s 81ms/step - loss: 1.0525 - accuracy: 0.6156 - val_loss: 0.9626 - val_accuracy: 0.6428\n",
      "Epoch 68/300\n",
      "785/785 [==============================] - 70s 89ms/step - loss: 1.0530 - accuracy: 0.6139 - val_loss: 0.9164 - val_accuracy: 0.6836\n",
      "Epoch 69/300\n",
      "785/785 [==============================] - 62s 79ms/step - loss: 1.0569 - accuracy: 0.6144 - val_loss: 0.9210 - val_accuracy: 0.6808\n",
      "Epoch 70/300\n",
      "785/785 [==============================] - 63s 80ms/step - loss: 1.0562 - accuracy: 0.6161 - val_loss: 0.9848 - val_accuracy: 0.6828\n",
      "Epoch 71/300\n",
      "785/785 [==============================] - 69s 88ms/step - loss: 1.0561 - accuracy: 0.6166 - val_loss: 0.9662 - val_accuracy: 0.6336\n",
      "Epoch 72/300\n",
      "785/785 [==============================] - 62s 78ms/step - loss: 1.0538 - accuracy: 0.6172 - val_loss: 1.0790 - val_accuracy: 0.5984\n",
      "Epoch 73/300\n",
      "785/785 [==============================] - 60s 76ms/step - loss: 1.0541 - accuracy: 0.6197 - val_loss: 0.9157 - val_accuracy: 0.6897\n",
      "Epoch 74/300\n",
      "785/785 [==============================] - 62s 79ms/step - loss: 1.0525 - accuracy: 0.6160 - val_loss: 0.9515 - val_accuracy: 0.6601\n",
      "Epoch 75/300\n",
      "785/785 [==============================] - 65s 82ms/step - loss: 1.0403 - accuracy: 0.6236 - val_loss: 0.9416 - val_accuracy: 0.6749\n",
      "Epoch 76/300\n",
      "785/785 [==============================] - 64s 81ms/step - loss: 1.0445 - accuracy: 0.6204 - val_loss: 0.9535 - val_accuracy: 0.6642\n",
      "Epoch 77/300\n",
      "785/785 [==============================] - 70s 89ms/step - loss: 1.0505 - accuracy: 0.6152 - val_loss: 0.9345 - val_accuracy: 0.6762\n",
      "Epoch 78/300\n",
      "785/785 [==============================] - 68s 87ms/step - loss: 1.0461 - accuracy: 0.6143 - val_loss: 0.9251 - val_accuracy: 0.6614\n",
      "Epoch 79/300\n",
      "785/785 [==============================] - 64s 81ms/step - loss: 1.0454 - accuracy: 0.6210 - val_loss: 0.9961 - val_accuracy: 0.6280\n",
      "Epoch 80/300\n",
      "785/785 [==============================] - 80s 102ms/step - loss: 1.0409 - accuracy: 0.6215 - val_loss: 1.0172 - val_accuracy: 0.6163\n",
      "Epoch 81/300\n",
      "785/785 [==============================] - 70s 89ms/step - loss: 1.0493 - accuracy: 0.6169 - val_loss: 0.9667 - val_accuracy: 0.6405\n",
      "Epoch 82/300\n",
      "785/785 [==============================] - 62s 79ms/step - loss: 1.0548 - accuracy: 0.6143 - val_loss: 0.9184 - val_accuracy: 0.6744\n",
      "Epoch 83/300\n",
      "785/785 [==============================] - 57s 72ms/step - loss: 1.0352 - accuracy: 0.6235 - val_loss: 0.8835 - val_accuracy: 0.6910\n",
      "Epoch 84/300\n",
      "785/785 [==============================] - 59s 75ms/step - loss: 1.0420 - accuracy: 0.6175 - val_loss: 1.0437 - val_accuracy: 0.6224\n",
      "Epoch 85/300\n",
      "785/785 [==============================] - 56s 72ms/step - loss: 1.0331 - accuracy: 0.6242 - val_loss: 0.9544 - val_accuracy: 0.6395\n",
      "Epoch 86/300\n",
      "785/785 [==============================] - 60s 77ms/step - loss: 1.0356 - accuracy: 0.6254 - val_loss: 0.9624 - val_accuracy: 0.6665\n",
      "Epoch 87/300\n",
      "785/785 [==============================] - 59s 75ms/step - loss: 1.0297 - accuracy: 0.6270 - val_loss: 0.9389 - val_accuracy: 0.6438\n",
      "Epoch 88/300\n",
      "785/785 [==============================] - 56s 71ms/step - loss: 1.0288 - accuracy: 0.6207 - val_loss: 1.0169 - val_accuracy: 0.6130\n",
      "Epoch 89/300\n",
      "785/785 [==============================] - 57s 72ms/step - loss: 1.0276 - accuracy: 0.6278 - val_loss: 0.9589 - val_accuracy: 0.6604\n",
      "Epoch 90/300\n",
      "785/785 [==============================] - 53s 68ms/step - loss: 1.0314 - accuracy: 0.6230 - val_loss: 0.9587 - val_accuracy: 0.6461\n",
      "Epoch 91/300\n",
      "785/785 [==============================] - 58s 74ms/step - loss: 1.0266 - accuracy: 0.6266 - val_loss: 0.9195 - val_accuracy: 0.6831\n",
      "Epoch 92/300\n",
      "785/785 [==============================] - 57s 73ms/step - loss: 1.0297 - accuracy: 0.6296 - val_loss: 0.9127 - val_accuracy: 0.6963\n",
      "Epoch 93/300\n",
      "785/785 [==============================] - 56s 72ms/step - loss: 1.0337 - accuracy: 0.6249 - val_loss: 0.9446 - val_accuracy: 0.6504\n",
      "Epoch 94/300\n",
      "785/785 [==============================] - 64s 81ms/step - loss: 1.0357 - accuracy: 0.6262 - val_loss: 0.9124 - val_accuracy: 0.6759\n",
      "Epoch 95/300\n",
      "785/785 [==============================] - 54s 69ms/step - loss: 1.0344 - accuracy: 0.6260 - val_loss: 1.0223 - val_accuracy: 0.6336\n",
      "Epoch 96/300\n",
      "785/785 [==============================] - 53s 68ms/step - loss: 1.0296 - accuracy: 0.6281 - val_loss: 0.9159 - val_accuracy: 0.6843\n",
      "Epoch 97/300\n",
      "785/785 [==============================] - 57s 73ms/step - loss: 1.0243 - accuracy: 0.6243 - val_loss: 0.9314 - val_accuracy: 0.6731\n",
      "Epoch 98/300\n",
      "785/785 [==============================] - 56s 72ms/step - loss: 1.0184 - accuracy: 0.6322 - val_loss: 0.9381 - val_accuracy: 0.6433\n",
      "Epoch 99/300\n",
      "785/785 [==============================] - 54s 68ms/step - loss: 1.0249 - accuracy: 0.6295 - val_loss: 0.9006 - val_accuracy: 0.6823\n",
      "Epoch 100/300\n",
      "785/785 [==============================] - 62s 79ms/step - loss: 1.0164 - accuracy: 0.6279 - val_loss: 1.1167 - val_accuracy: 0.6362\n",
      "Epoch 101/300\n",
      "785/785 [==============================] - 58s 73ms/step - loss: 1.0270 - accuracy: 0.6306 - val_loss: 1.1353 - val_accuracy: 0.5762\n",
      "Epoch 102/300\n",
      "785/785 [==============================] - 62s 78ms/step - loss: 1.0270 - accuracy: 0.6260 - val_loss: 0.9912 - val_accuracy: 0.6265\n",
      "Epoch 103/300\n",
      "785/785 [==============================] - 58s 74ms/step - loss: 1.0255 - accuracy: 0.6283 - val_loss: 1.0625 - val_accuracy: 0.6364\n",
      "Epoch 104/300\n",
      "785/785 [==============================] - 57s 72ms/step - loss: 1.0218 - accuracy: 0.6268 - val_loss: 0.9367 - val_accuracy: 0.6611\n",
      "Epoch 105/300\n",
      "785/785 [==============================] - 57s 72ms/step - loss: 1.0205 - accuracy: 0.6338 - val_loss: 1.0244 - val_accuracy: 0.6685\n",
      "Epoch 106/300\n",
      "785/785 [==============================] - 58s 73ms/step - loss: 1.0228 - accuracy: 0.6292 - val_loss: 0.9991 - val_accuracy: 0.6331\n",
      "Epoch 107/300\n",
      "785/785 [==============================] - 57s 73ms/step - loss: 1.0229 - accuracy: 0.6276 - val_loss: 0.8879 - val_accuracy: 0.6851\n",
      "Epoch 108/300\n",
      "785/785 [==============================] - 58s 74ms/step - loss: 1.0101 - accuracy: 0.6353 - val_loss: 0.9334 - val_accuracy: 0.6665\n",
      "Epoch 109/300\n",
      "785/785 [==============================] - 56s 71ms/step - loss: 1.0216 - accuracy: 0.6279 - val_loss: 0.9120 - val_accuracy: 0.6632\n",
      "Epoch 110/300\n",
      "785/785 [==============================] - 49s 62ms/step - loss: 1.0104 - accuracy: 0.6354 - val_loss: 0.9408 - val_accuracy: 0.6430\n",
      "Epoch 111/300\n",
      "785/785 [==============================] - 46s 59ms/step - loss: 1.0169 - accuracy: 0.6294 - val_loss: 0.8943 - val_accuracy: 0.6782\n",
      "Epoch 112/300\n",
      "785/785 [==============================] - 48s 61ms/step - loss: 1.0170 - accuracy: 0.6294 - val_loss: 1.1467 - val_accuracy: 0.6430\n",
      "Epoch 113/300\n",
      "785/785 [==============================] - 51s 64ms/step - loss: 1.0127 - accuracy: 0.6317 - val_loss: 0.8692 - val_accuracy: 0.7040\n",
      "Epoch 114/300\n",
      "785/785 [==============================] - 49s 62ms/step - loss: 1.0139 - accuracy: 0.6320 - val_loss: 0.9039 - val_accuracy: 0.6810\n",
      "Epoch 115/300\n",
      "785/785 [==============================] - 51s 65ms/step - loss: 1.0166 - accuracy: 0.6302 - val_loss: 0.9238 - val_accuracy: 0.6611\n",
      "Epoch 116/300\n",
      "785/785 [==============================] - 50s 63ms/step - loss: 1.0072 - accuracy: 0.6358 - val_loss: 0.8859 - val_accuracy: 0.6739\n",
      "Epoch 117/300\n",
      "785/785 [==============================] - 49s 62ms/step - loss: 1.0151 - accuracy: 0.6336 - val_loss: 0.9736 - val_accuracy: 0.6341\n",
      "Epoch 118/300\n",
      "785/785 [==============================] - 52s 67ms/step - loss: 1.0098 - accuracy: 0.6335 - val_loss: 0.9146 - val_accuracy: 0.6688\n",
      "Epoch 119/300\n",
      "785/785 [==============================] - 51s 66ms/step - loss: 1.0086 - accuracy: 0.6353 - val_loss: 0.9267 - val_accuracy: 0.6685\n",
      "Epoch 120/300\n",
      "785/785 [==============================] - 57s 73ms/step - loss: 1.0057 - accuracy: 0.6316 - val_loss: 0.9658 - val_accuracy: 0.6395\n",
      "Epoch 121/300\n",
      "785/785 [==============================] - 60s 76ms/step - loss: 1.0033 - accuracy: 0.6337 - val_loss: 1.0199 - val_accuracy: 0.6356\n",
      "Epoch 122/300\n",
      "785/785 [==============================] - 61s 78ms/step - loss: 1.0049 - accuracy: 0.6396 - val_loss: 0.9636 - val_accuracy: 0.6402\n",
      "Epoch 123/300\n",
      "785/785 [==============================] - 64s 82ms/step - loss: 1.0037 - accuracy: 0.6388 - val_loss: 1.0052 - val_accuracy: 0.6203\n",
      "Epoch 124/300\n",
      "785/785 [==============================] - 61s 78ms/step - loss: 1.0132 - accuracy: 0.6315 - val_loss: 1.0120 - val_accuracy: 0.6645\n",
      "Epoch 125/300\n",
      "785/785 [==============================] - 62s 79ms/step - loss: 1.0023 - accuracy: 0.6359 - val_loss: 0.8987 - val_accuracy: 0.6747\n",
      "Epoch 126/300\n",
      "785/785 [==============================] - 60s 76ms/step - loss: 1.0009 - accuracy: 0.6371 - val_loss: 0.8488 - val_accuracy: 0.7073\n",
      "Epoch 127/300\n",
      "785/785 [==============================] - 53s 67ms/step - loss: 0.9969 - accuracy: 0.6421 - val_loss: 1.0331 - val_accuracy: 0.6290\n",
      "Epoch 128/300\n",
      "785/785 [==============================] - 56s 71ms/step - loss: 1.0026 - accuracy: 0.6352 - val_loss: 0.9310 - val_accuracy: 0.6741\n",
      "Epoch 129/300\n",
      "785/785 [==============================] - 69s 87ms/step - loss: 1.0012 - accuracy: 0.6349 - val_loss: 0.9059 - val_accuracy: 0.6701\n",
      "Epoch 130/300\n",
      "785/785 [==============================] - 68s 86ms/step - loss: 1.0097 - accuracy: 0.6354 - val_loss: 0.8925 - val_accuracy: 0.6823\n",
      "Epoch 131/300\n",
      "785/785 [==============================] - 65s 83ms/step - loss: 1.0067 - accuracy: 0.6350 - val_loss: 0.9812 - val_accuracy: 0.6509\n",
      "Epoch 132/300\n",
      "785/785 [==============================] - 71s 90ms/step - loss: 1.0076 - accuracy: 0.6330 - val_loss: 1.0323 - val_accuracy: 0.6084\n",
      "Epoch 133/300\n",
      "785/785 [==============================] - 65s 83ms/step - loss: 1.0055 - accuracy: 0.6368 - val_loss: 0.9016 - val_accuracy: 0.6782\n",
      "Epoch 134/300\n",
      "785/785 [==============================] - 65s 82ms/step - loss: 1.0125 - accuracy: 0.6333 - val_loss: 0.9622 - val_accuracy: 0.6359\n",
      "Epoch 135/300\n",
      "785/785 [==============================] - 71s 91ms/step - loss: 0.9974 - accuracy: 0.6377 - val_loss: 1.0122 - val_accuracy: 0.6260\n",
      "Epoch 136/300\n",
      "785/785 [==============================] - 65s 83ms/step - loss: 0.9964 - accuracy: 0.6398 - val_loss: 0.9042 - val_accuracy: 0.6764\n",
      "Epoch 137/300\n",
      "785/785 [==============================] - 71s 90ms/step - loss: 1.0075 - accuracy: 0.6338 - val_loss: 0.9910 - val_accuracy: 0.6379\n",
      "Epoch 138/300\n",
      "785/785 [==============================] - 66s 83ms/step - loss: 1.0032 - accuracy: 0.6334 - val_loss: 1.0263 - val_accuracy: 0.6178\n",
      "Epoch 139/300\n",
      "785/785 [==============================] - 71s 91ms/step - loss: 0.9986 - accuracy: 0.6371 - val_loss: 0.9426 - val_accuracy: 0.6770\n",
      "Epoch 140/300\n",
      "785/785 [==============================] - 66s 84ms/step - loss: 1.0017 - accuracy: 0.6400 - val_loss: 1.0683 - val_accuracy: 0.6321\n",
      "Epoch 141/300\n",
      "785/785 [==============================] - 69s 88ms/step - loss: 0.9937 - accuracy: 0.6415 - val_loss: 0.9186 - val_accuracy: 0.6762\n",
      "Epoch 142/300\n",
      "785/785 [==============================] - 69s 88ms/step - loss: 1.0039 - accuracy: 0.6336 - val_loss: 0.8775 - val_accuracy: 0.6843\n",
      "Epoch 143/300\n",
      "785/785 [==============================] - 65s 83ms/step - loss: 0.9899 - accuracy: 0.6428 - val_loss: 0.9270 - val_accuracy: 0.6690\n",
      "Epoch 144/300\n",
      "785/785 [==============================] - 70s 89ms/step - loss: 0.9958 - accuracy: 0.6401 - val_loss: 0.8738 - val_accuracy: 0.6851\n",
      "Epoch 145/300\n",
      "785/785 [==============================] - 68s 86ms/step - loss: 0.9994 - accuracy: 0.6407 - val_loss: 0.8545 - val_accuracy: 0.7060\n",
      "Epoch 146/300\n",
      "785/785 [==============================] - 68s 86ms/step - loss: 1.0032 - accuracy: 0.6341 - val_loss: 0.9367 - val_accuracy: 0.6522\n",
      "Epoch 147/300\n",
      "785/785 [==============================] - 65s 82ms/step - loss: 0.9904 - accuracy: 0.6403 - val_loss: 0.9673 - val_accuracy: 0.6400\n",
      "Epoch 148/300\n",
      "785/785 [==============================] - 69s 88ms/step - loss: 0.9965 - accuracy: 0.6383 - val_loss: 0.8785 - val_accuracy: 0.6902\n",
      "Epoch 149/300\n",
      "785/785 [==============================] - 65s 83ms/step - loss: 0.9938 - accuracy: 0.6381 - val_loss: 0.9609 - val_accuracy: 0.6382\n",
      "Epoch 150/300\n",
      "785/785 [==============================] - 69s 88ms/step - loss: 0.9983 - accuracy: 0.6387 - val_loss: 0.9718 - val_accuracy: 0.6448\n",
      "Epoch 151/300\n",
      "785/785 [==============================] - 65s 82ms/step - loss: 0.9919 - accuracy: 0.6425 - val_loss: 0.8957 - val_accuracy: 0.6757\n",
      "Epoch 152/300\n",
      "785/785 [==============================] - 83s 106ms/step - loss: 0.9920 - accuracy: 0.6415 - val_loss: 0.9597 - val_accuracy: 0.6425\n",
      "Epoch 153/300\n",
      "785/785 [==============================] - 76s 97ms/step - loss: 0.9895 - accuracy: 0.6400 - val_loss: 0.9181 - val_accuracy: 0.6706\n",
      "Epoch 154/300\n",
      "785/785 [==============================] - 71s 90ms/step - loss: 0.9899 - accuracy: 0.6385 - val_loss: 0.8813 - val_accuracy: 0.6869\n",
      "Epoch 155/300\n",
      "785/785 [==============================] - 56s 72ms/step - loss: 0.9864 - accuracy: 0.6468 - val_loss: 0.8988 - val_accuracy: 0.6650\n",
      "Epoch 156/300\n",
      "785/785 [==============================] - 68s 87ms/step - loss: 0.9926 - accuracy: 0.6391 - val_loss: 1.0013 - val_accuracy: 0.6234\n",
      "Epoch 157/300\n",
      "389/785 [=============>................] - ETA: 45s - loss: 1.0065 - accuracy: 0.6349"
     ]
    }
   ],
   "source": [
    "# steps_per_epoch: number of batches to be drawn from the generator after assuming epoch over\n",
    "# epochs: number of epochs\n",
    "# validation_steps: how many batches to draw from the validation generator for evaluation\n",
    "\n",
    "# TODO: set to the number of images we have\n",
    "number_training = len(X_train)\n",
    "number_eval = len(X_eval)\n",
    "\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=int(math.ceil((1. * number_training) / batch_size)),\n",
    "      epochs=epochs,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=int(math.ceil((1. * number_eval) / batch_size)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Visualization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation ACC')\n",
    "plt.legend()\n",
    "plt.figure()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# smooth curves if they look noisy\n",
    "# replace each point with an exponential moving average of the previous points\n",
    "def smooth_curve(points, factor=0.8):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(epochs,\n",
    "         smooth_curve(acc), 'r', label='Smoothed training acc')\n",
    "plt.plot(epochs,\n",
    "         smooth_curve(val_acc), 'b', label='Smoothed validation acc')\n",
    "plt.title('Training and validation MAE')\n",
    "plt.legend()\n",
    "plt.figure()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(epochs,\n",
    "         smooth_curve(loss), 'r', label='Smoothed training loss')\n",
    "plt.plot(epochs,\n",
    "         smooth_curve(val_loss), 'b', label='Smoothed validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# display average, the model may improve even if not reflected"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Early stopping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "\n",
    "callbacks_list = [\n",
    "        # interrupts training when accuracy has stopped improving accuracy on the validation set for at least 3+1=4 epochs\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='acc', # should be part of the metrics specific during compilation\n",
    "            patience=10,\n",
    "        ),\n",
    "        # save the current weights after every epoch\n",
    "        #keras.callbacks.ModelCheckpoint(\n",
    "        #    filepath=os.path.join(paths['MODELS'],'CNN_baseline.h5'),\n",
    "        #    monitor='val_loss', # do not overwrite until val_loss is improved\n",
    "        #    save_best_only=True\n",
    "        #),\n",
    "        # monitor the model's validation loss and reduce the LR when the validation loss has stopped improving, effective strategy to escape local minima\n",
    "        #keras.callbacks.ReduceLROnPlateau(\n",
    "        #    monitor='val_loss',\n",
    "        #    factor=0.2, # divides LR by 5 when triggered\n",
    "        #    patience=3 # called when stopped improving for 3 epochs\n",
    "        #),\n",
    "        #keras.callbacks.TensorBoard(\n",
    "        #    log_dir=paths['LOG_DIR'],\n",
    "        #    write_graph=True,\n",
    "        #    histogram_freq=1 # record activation histograms every 1 epoch\n",
    "        #)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=3,\n",
    "      epochs=epochs,\n",
    "      validation_data=validation_generator,\n",
    "      callbacks=callbacks_list,\n",
    "      validation_steps=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Model testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dir=paths[\"TEST_PATH\"]\n",
    "test_datagen = ImageDataGenerator(rescale=rescale_size) # it should not be augmented\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# if performances are much wors than validation ones, during hyperparameter optimization (when done) the process has overfitted the validdation set, if so go to a more clear protocol such as Kfold CV\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=2)\n",
    "print('test acc:', test_acc)\n",
    "print('test loss:', test_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: is it balanced? If not consider ROC AUC for example, FPR, TPR, and others"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Model exportation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save(\"models/CNN_baseline.h5\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Plot model as graph of layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, to_file='model.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
