{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# feature extraction consists of taking the convolutional base of a pre-existing network and stick a new densely connected layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 14:36:37.245301: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/francoterranova/opt/anaconda3/lib/python3.9/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/Users/francoterranova/opt/anaconda3/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: [\"dlopen(/Users/francoterranova/opt/anaconda3/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so, 0x0006): symbol not found in flat namespace '__ZN3tsl2io7DirnameENSt3__117basic_string_viewIcNS1_11char_traitsIcEEEE'\"]\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/Users/francoterranova/opt/anaconda3/lib/python3.9/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/Users/francoterranova/opt/anaconda3/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: [\"dlopen(/Users/francoterranova/opt/anaconda3/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io.so, 0x0006): symbol not found in flat namespace '__ZN10tensorflow12OpDefBuilder10SetShapeFnENSt3__18functionIFN3tsl6StatusEPNS_15shape_inference16InferenceContextEEEE'\"]\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16 # TODO: try also Xception, Inception V3, ResNet50, VGG16, VGG19, MobileNet and others domain-specific nets\n",
    "from keras.applications import Xception\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications import EfficientNetB2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# hyperparameters to be set\n",
    "input_height = 1920\n",
    "input_width = 1080\n",
    "# others"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "conv_base = VGG16(weights='imagenet', # weight checkpoint from which we initialize the model\n",
    "                  include_top=False, # to decide if need to include the densely connectedd layer\n",
    "                  input_shape=(input_width, input_height, 3)) #TODO: check order"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1080, 1920, 3)]   0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 1080, 1920, 64)    1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 1080, 1920, 64)    36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 540, 960, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 540, 960, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 540, 960, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 270, 480, 128)     0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 270, 480, 256)     295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 270, 480, 256)     590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 270, 480, 256)     590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 135, 240, 256)     0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 135, 240, 512)     1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 135, 240, 512)     2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 135, 240, 512)     2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 67, 120, 512)      0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 67, 120, 512)      2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 67, 120, 512)      2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 67, 120, 512)      2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 33, 60, 512)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. First approach: run convolutional base over inputs, save in array and then give it to a standalone densely connected layer (approach cannot use data augmentation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "images_path = os.path.join(\"..\",\"workspace\",\"images\")\n",
    "train_dir = os.path.join(images_path, \"train\")\n",
    "validation_dir = os.path.join(images_path, \"eval\")\n",
    "test_dir = os.path.join(images_path, \"test\")\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512)) # (4,4,512) is the VGG16 final feature map shape\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    generator = datagen.flow_from_directory(directory, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')\n",
    "    i=0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        # we extract features by calling the predict method of the conv_base model\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 153 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for filename in os.listdir(train_dir):\n",
    "    count += 1\n",
    "train_features, train_labels = extract_features(train_dir, count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count = 0\n",
    "for filename in os.listdir(validation_dir):\n",
    "    count += 1\n",
    "validation_features, validation_labels = extract_features(validation_dir, count)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count = 0\n",
    "for filename in os.listdir(test_dir):\n",
    "    count += 1\n",
    "test_features, test_labels = extract_features(test_dir, count)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best loss function for multi-class classification, measures the distance between two probability distributions\n",
    "# the probability distribution of the output of the network and the true distribution of the labels\n",
    "loss_function='categorical_crossentropy'\n",
    "\n",
    "metrics=['accuracy']\n",
    "optimizer='rmsprop'\n",
    "optimizer_learning_rate=1e-4\n",
    "epochs=5\n",
    "batch_size=20\n",
    "num_classes = 8\n",
    "\n",
    "if optimizer == 'rmsprop':\n",
    "    optimizer=optimizers.RMSprop(learning_rate=optimizer_learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# densely connected classifier on top\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_function,\n",
    "              metrics=metrics)\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation ACC')\n",
    "plt.legend()\n",
    "plt.figure()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Approach that extend the model and add a dense layer on top, running the whole thing end to end on the input data (support data augmentation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "images_path = os.path.join(\"..\",\"workspace\",\"images\")\n",
    "train_dir = os.path.join(images_path, \"train\")\n",
    "validation_dir = os.path.join(images_path, \"eval\")\n",
    "test_dir = os.path.join(images_path, \"test\")\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 33, 60, 512)       14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1013760)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               259522816 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 274,237,761\n",
      "Trainable params: 274,237,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of trainable weights before freezing the conv base: 30\n"
     ]
    }
   ],
   "source": [
    "print('This is the number of trainable weights '\n",
    "         'before freezing the conv base:', len(model.trainable_weights))\n",
    "conv_base.trainable = False # freeze conv_base otherwise representation previously learned got updated\n",
    "# only the weights of the densely connected layer will be trained"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of trainable weights after freezing the conv base: 4\n"
     ]
    }
   ],
   "source": [
    "print('This is the number of trainable weights '\n",
    "          'after freezing the conv base:', len(model.trainable_weights))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best loss function for multi-class classification, measures the distance between two probability distributions\n",
    "# the probability distribution of the output of the network and the true distribution of the labels\n",
    "loss_function='categorical_crossentropy'\n",
    "\n",
    "metrics=['accuracy']\n",
    "optimizer='rmsprop'\n",
    "optimizer_learning_rate=1e-4\n",
    "epochs=5\n",
    "batch_size=20\n",
    "num_classes = 8\n",
    "\n",
    "if optimizer == 'rmsprop':\n",
    "    optimizer=optimizers.RMSprop(learning_rate=optimizer_learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 153 validated image filenames.\n",
      "Found 24 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francoterranova/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "/var/folders/lp/fbpz2wyx1kb4b5zgt8w_tsp40000gn/T/ipykernel_50095/3850630360.py:22: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest') # set right parameters\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "df_eval = pd.read_csv(os.path.join(validation_dir, \"df_eval.csv\"))\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics)\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=10,\n",
    "      epochs=3,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation MAE')\n",
    "plt.legend()\n",
    "plt.figure()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255) # it should not be augmented\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255) # it should not be augmented\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# if performances are much wors than validation ones, during hyperparameter optimization (when done) the process has overfitted the validdation set, if so go to a more clear protocol such as Kfold CV\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=2)\n",
    "print('test acc:', test_acc)\n",
    "print('test loss:', test_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
