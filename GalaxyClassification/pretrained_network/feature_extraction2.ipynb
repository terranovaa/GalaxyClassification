{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 14:06:58.844153: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras as keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.optimizers as optimizers\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "input_height = 69\n",
    "input_width = 69\n",
    "batch_size = 64\n",
    "\n",
    "# TODO: find best parameters using the display_data_augmentation_sample jupyter notebook\n",
    "rescale = True\n",
    "if rescale:\n",
    "    rescale_size=1./255\n",
    "else:\n",
    "    rescale_size=1\n",
    "augmentation=False\n",
    "\n",
    "rotation_range=40\n",
    "width_shift_range=0.2\n",
    "height_shift_range=0.1\n",
    "shear_range=0.2\n",
    "zoom_range=0.2\n",
    "horizontal_flip=True\n",
    "fill_mode='nearest'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 14:07:22.620185: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "# best loss function for multi-class classification, measures the distance between two probability distributions\n",
    "# the probability distribution of the output of the network and the true distribution of the labels\n",
    "loss_function='categorical_crossentropy'\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "metrics = [\n",
    "    precision,\n",
    "    recall,\n",
    "    tf.keras.metrics.CategoricalAccuracy(name='acc')\n",
    "]\n",
    "optimizer='rmsprop'\n",
    "optimizer_learning_rate=1e-4\n",
    "epochs=100\n",
    "batch_size=32\n",
    "regularizer=regularizers.l1_l2(l1=0.001, l2=0.001) # simultaneous l1 and l2, add 0.001*weight_coefficient_value + 0.001 * 1/2*weight^2\n",
    "\n",
    "if optimizer == 'rmsprop':\n",
    "    optimizer=optimizers.RMSprop(learning_rate=optimizer_learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'TRAIN_PATH' : os.path.join('..','workspace', 'images', 'train'),\n",
    "    'TEST_PATH' : os.path.join('..','workspace', 'images','test'),\n",
    "    'EVAL_PATH' : os.path.join('..','workspace', 'images','eval'),\n",
    "    'IMAGES_PATH': os.path.join('..','workspace','images','all'),\n",
    "    'LOG_DIR' : os.path.join('..','model', 'log_dir')\n",
    " }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2488, 1: 3000, 2: 3000, 3: 858, 4: 1523, 5: 1016, 6: 1298, 7: 1146, 8: 981}\n"
     ]
    }
   ],
   "source": [
    "def plotTrainingDistribution():\n",
    "    files_per_label = dict()\n",
    "    for i in range(9):\n",
    "      path = os.path.join(paths['TRAIN_PATH'],str(i))\n",
    "      n_images = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "      files_per_label[i] = n_images\n",
    "    plt.bar(list(files_per_label.keys()), files_per_label.values(), color='g')\n",
    "    plt.show()\n",
    "    print(files_per_label)\n",
    "    return files_per_label\n",
    "\n",
    "files_per_label = plotTrainingDistribution()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15310\n",
      "{0: 0.6837263308324402, 1: 0.567037037037037, 2: 0.567037037037037, 3: 1.9826469826469826, 4: 1.1169475450499744, 5: 1.6743219597550307, 6: 1.3105632597158021, 7: 1.4843901493116152, 8: 1.7340582172386454}\n"
     ]
    }
   ],
   "source": [
    "#weight computation\n",
    "\n",
    "#number of classes\n",
    "NUM_CLASSES = 9\n",
    "\n",
    "#get number of total images\n",
    "tot_images = sum(list(files_per_label.values()))\n",
    "print(tot_images)\n",
    "\n",
    "#dictionary storing weights for each class\n",
    "#weight[i] = number_total_samples / number_total_classes * num_samples_class_i\n",
    "weights = dict([ (class_label , tot_images/(NUM_CLASSES * n_images)) for class_label, n_images in files_per_label.items()])\n",
    "\n",
    "print(weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15310 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "# training set image data generator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "if augmentation:\n",
    "    train_datagen = ImageDataGenerator(\n",
    "          rescale=rescale_size,\n",
    "          rotation_range=rotation_range,\n",
    "          width_shift_range=width_shift_range,\n",
    "          height_shift_range=height_shift_range,\n",
    "          shear_range=shear_range,\n",
    "          zoom_range=zoom_range,\n",
    "          horizontal_flip=horizontal_flip,\n",
    "          fill_mode=fill_mode)\n",
    "else:\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    # to perform normalization we should never use information coming from the test set, only training set\n",
    "\n",
    "train_dir=paths['TRAIN_PATH']\n",
    "\n",
    "# TODO: Consider if the output should be normalized\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3919 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "# validation set image data generator\n",
    "val_datagen = ImageDataGenerator(rescale=rescale_size) # it should not be augmented\n",
    "\n",
    "validation_dir=paths['EVAL_PATH']\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(validation_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Model creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from keras.applications import EfficientNetB2\n",
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                    include_top=False,\n",
    "                    input_shape=(input_width, input_height, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def build_model(input_conv_base):\n",
    "    #build the cnn using the pre-trained cnn\n",
    "    built_model = models.Sequential()\n",
    "    built_model.add(input_conv_base)\n",
    "    built_model.add(Flatten())\n",
    "    built_model.add(Dense(64, activation='relu'))\n",
    "    built_model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "    return built_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 69, 69, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 69, 69, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 69, 69, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 34, 34, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 34, 34, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 17, 17, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 17, 17, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 17, 17, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 17, 17, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "conv_base.trainable = False # freeze conv_base otherwise representation previously learned got updated\n",
    "# only the weights of the densely connected layer will be trained"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 69, 69, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 69, 69, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 69, 69, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 34, 34, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 34, 34, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 17, 17, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 17, 17, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 17, 17, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 17, 17, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#build model\n",
    "model = build_model(conv_base)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_function,\n",
    "              metrics=metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 2, 2, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                131136    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 9)                 585       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,846,409\n",
      "Trainable params: 131,721\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Model training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3919\n"
     ]
    }
   ],
   "source": [
    "n_images_eval = 0\n",
    "for i in range(9):\n",
    "    path = os.path.join(paths['EVAL_PATH'],str(i))\n",
    "    #compute number of images in each eval folder and sum it up\n",
    "    n_images_eval = n_images_eval + len([f for f in os.listdir(path)if os.path.isfile(os.path.join(path, f))])\n",
    "\n",
    "print(n_images_eval)\n",
    "# TODO: set to the number of images we have\n",
    "number_training = tot_images # TODO: contare elementi training cartella\n",
    "number_eval = n_images_eval"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "479/479 [==============================] - 653s 1s/step - loss: 1.7667 - precision: 0.3177 - recall: 0.0198 - acc: 0.3751 - val_loss: 1.5438 - val_precision: 0.4472 - val_recall: 0.0211 - val_acc: 0.4797 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "479/479 [==============================] - 1231s 3s/step - loss: 1.4127 - precision: 0.7086 - recall: 0.0745 - acc: 0.4668 - val_loss: 1.3559 - val_precision: 0.7108 - val_recall: 0.0597 - val_acc: 0.5142 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "479/479 [==============================] - 538s 1s/step - loss: 1.2814 - precision: 0.7547 - recall: 0.1373 - acc: 0.5054 - val_loss: 1.2741 - val_precision: 0.7592 - val_recall: 0.1324 - val_acc: 0.5272 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "479/479 [==============================] - 483s 1s/step - loss: 1.2004 - precision: 0.7490 - recall: 0.1872 - acc: 0.5347 - val_loss: 1.2183 - val_precision: 0.8267 - val_recall: 0.1327 - val_acc: 0.5532 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "479/479 [==============================] - 399s 834ms/step - loss: 1.1447 - precision: 0.7558 - recall: 0.2287 - acc: 0.5586 - val_loss: 1.1360 - val_precision: 0.7957 - val_recall: 0.2352 - val_acc: 0.5734 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "479/479 [==============================] - 397s 829ms/step - loss: 1.1033 - precision: 0.7544 - recall: 0.2647 - acc: 0.5677 - val_loss: 1.1088 - val_precision: 0.7782 - val_recall: 0.2666 - val_acc: 0.5787 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "479/479 [==============================] - 438s 914ms/step - loss: 1.0703 - precision: 0.7522 - recall: 0.2947 - acc: 0.5818 - val_loss: 1.0446 - val_precision: 0.7792 - val_recall: 0.3068 - val_acc: 0.6014 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "479/479 [==============================] - 412s 861ms/step - loss: 1.0449 - precision: 0.7501 - recall: 0.3185 - acc: 0.5904 - val_loss: 1.0558 - val_precision: 0.7857 - val_recall: 0.3078 - val_acc: 0.5973 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "479/479 [==============================] - 494s 1s/step - loss: 1.0229 - precision: 0.7540 - recall: 0.3397 - acc: 0.6009 - val_loss: 1.0113 - val_precision: 0.7671 - val_recall: 0.3417 - val_acc: 0.6081 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "479/479 [==============================] - 524s 1s/step - loss: 1.0036 - precision: 0.7519 - recall: 0.3600 - acc: 0.6075 - val_loss: 0.9897 - val_precision: 0.7651 - val_recall: 0.3569 - val_acc: 0.6234 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "479/479 [==============================] - 521s 1s/step - loss: 0.9868 - precision: 0.7496 - recall: 0.3762 - acc: 0.6157 - val_loss: 0.9983 - val_precision: 0.7702 - val_recall: 0.3612 - val_acc: 0.6167 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "479/479 [==============================] - 442s 922ms/step - loss: 0.9719 - precision: 0.7539 - recall: 0.3960 - acc: 0.6234 - val_loss: 0.9875 - val_precision: 0.7644 - val_recall: 0.3861 - val_acc: 0.6246 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "479/479 [==============================] - 457s 955ms/step - loss: 0.9588 - precision: 0.7470 - recall: 0.4084 - acc: 0.6269 - val_loss: 0.9630 - val_precision: 0.7582 - val_recall: 0.3981 - val_acc: 0.6292 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "479/479 [==============================] - 532s 1s/step - loss: 0.9469 - precision: 0.7531 - recall: 0.4191 - acc: 0.6305 - val_loss: 0.9388 - val_precision: 0.7622 - val_recall: 0.4301 - val_acc: 0.6379 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "479/479 [==============================] - 569s 1s/step - loss: 0.9358 - precision: 0.7507 - recall: 0.4306 - acc: 0.6327 - val_loss: 0.9504 - val_precision: 0.7593 - val_recall: 0.4318 - val_acc: 0.6298 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "479/479 [==============================] - 524s 1s/step - loss: 0.9264 - precision: 0.7545 - recall: 0.4416 - acc: 0.6366 - val_loss: 0.9222 - val_precision: 0.7682 - val_recall: 0.4405 - val_acc: 0.6466 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "479/479 [==============================] - 456s 952ms/step - loss: 0.9163 - precision: 0.7542 - recall: 0.4489 - acc: 0.6388 - val_loss: 0.9543 - val_precision: 0.7586 - val_recall: 0.4191 - val_acc: 0.6338 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "479/479 [==============================] - 510s 1s/step - loss: 0.9069 - precision: 0.7527 - recall: 0.4598 - acc: 0.6447 - val_loss: 0.9421 - val_precision: 0.7487 - val_recall: 0.4281 - val_acc: 0.6356 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "479/479 [==============================] - 522s 1s/step - loss: 0.8984 - precision: 0.7559 - recall: 0.4695 - acc: 0.6489 - val_loss: 0.9353 - val_precision: 0.7554 - val_recall: 0.4398 - val_acc: 0.6407 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "479/479 [==============================] - 434s 906ms/step - loss: 0.8844 - precision: 0.7640 - recall: 0.4724 - acc: 0.6570 - val_loss: 0.9024 - val_precision: 0.7691 - val_recall: 0.4730 - val_acc: 0.6540 - lr: 2.0000e-05\n",
      "Epoch 21/100\n",
      "479/479 [==============================] - 404s 843ms/step - loss: 0.8827 - precision: 0.7620 - recall: 0.4761 - acc: 0.6560 - val_loss: 0.9089 - val_precision: 0.7680 - val_recall: 0.4653 - val_acc: 0.6530 - lr: 2.0000e-05\n",
      "Epoch 22/100\n",
      "479/479 [==============================] - 429s 896ms/step - loss: 0.8812 - precision: 0.7627 - recall: 0.4770 - acc: 0.6569 - val_loss: 0.9039 - val_precision: 0.7627 - val_recall: 0.4659 - val_acc: 0.6527 - lr: 2.0000e-05\n",
      "Epoch 23/100\n",
      "479/479 [==============================] - 460s 960ms/step - loss: 0.8797 - precision: 0.7637 - recall: 0.4789 - acc: 0.6586 - val_loss: 0.9021 - val_precision: 0.7658 - val_recall: 0.4654 - val_acc: 0.6530 - lr: 2.0000e-05\n",
      "Epoch 24/100\n",
      "479/479 [==============================] - 456s 951ms/step - loss: 0.8783 - precision: 0.7646 - recall: 0.4788 - acc: 0.6587 - val_loss: 0.8997 - val_precision: 0.7661 - val_recall: 0.4691 - val_acc: 0.6548 - lr: 2.0000e-05\n",
      "Epoch 25/100\n",
      "479/479 [==============================] - 438s 916ms/step - loss: 0.8765 - precision: 0.7646 - recall: 0.4822 - acc: 0.6583 - val_loss: 0.8962 - val_precision: 0.7583 - val_recall: 0.4708 - val_acc: 0.6560 - lr: 2.0000e-05\n",
      "Epoch 26/100\n",
      "479/479 [==============================] - 411s 857ms/step - loss: 0.8749 - precision: 0.7668 - recall: 0.4850 - acc: 0.6590 - val_loss: 0.9004 - val_precision: 0.7631 - val_recall: 0.4717 - val_acc: 0.6535 - lr: 2.0000e-05\n",
      "Epoch 27/100\n",
      "479/479 [==============================] - 354s 739ms/step - loss: 0.8732 - precision: 0.7620 - recall: 0.4838 - acc: 0.6590 - val_loss: 0.8929 - val_precision: 0.7688 - val_recall: 0.4785 - val_acc: 0.6583 - lr: 2.0000e-05\n",
      "Epoch 28/100\n",
      "479/479 [==============================] - 364s 760ms/step - loss: 0.8721 - precision: 0.7649 - recall: 0.4859 - acc: 0.6607 - val_loss: 0.8938 - val_precision: 0.7625 - val_recall: 0.4806 - val_acc: 0.6565 - lr: 2.0000e-05\n",
      "Epoch 29/100\n",
      "479/479 [==============================] - 372s 778ms/step - loss: 0.8704 - precision: 0.7657 - recall: 0.4888 - acc: 0.6602 - val_loss: 0.8976 - val_precision: 0.7652 - val_recall: 0.4746 - val_acc: 0.6535 - lr: 2.0000e-05\n",
      "Epoch 30/100\n",
      "479/479 [==============================] - 440s 919ms/step - loss: 0.8690 - precision: 0.7674 - recall: 0.4883 - acc: 0.6604 - val_loss: 0.8896 - val_precision: 0.7673 - val_recall: 0.4830 - val_acc: 0.6604 - lr: 2.0000e-05\n",
      "Epoch 31/100\n",
      "479/479 [==============================] - 552s 1s/step - loss: 0.8675 - precision: 0.7647 - recall: 0.4890 - acc: 0.6640 - val_loss: 0.8907 - val_precision: 0.7673 - val_recall: 0.4832 - val_acc: 0.6588 - lr: 2.0000e-05\n",
      "Epoch 32/100\n",
      "479/479 [==============================] - 645s 1s/step - loss: 0.8662 - precision: 0.7665 - recall: 0.4922 - acc: 0.6633 - val_loss: 0.8870 - val_precision: 0.7679 - val_recall: 0.4798 - val_acc: 0.6591 - lr: 2.0000e-05\n",
      "Epoch 33/100\n",
      "479/479 [==============================] - 596s 1s/step - loss: 0.8648 - precision: 0.7665 - recall: 0.4932 - acc: 0.6639 - val_loss: 0.8866 - val_precision: 0.7678 - val_recall: 0.4838 - val_acc: 0.6596 - lr: 2.0000e-05\n",
      "Epoch 34/100\n",
      "479/479 [==============================] - 734s 2s/step - loss: 0.8633 - precision: 0.7681 - recall: 0.4938 - acc: 0.6626 - val_loss: 0.8949 - val_precision: 0.7604 - val_recall: 0.4775 - val_acc: 0.6568 - lr: 2.0000e-05\n",
      "Epoch 35/100\n",
      "479/479 [==============================] - 734s 2s/step - loss: 0.8619 - precision: 0.7672 - recall: 0.4964 - acc: 0.6627 - val_loss: 0.8922 - val_precision: 0.7636 - val_recall: 0.4824 - val_acc: 0.6604 - lr: 2.0000e-05\n",
      "Epoch 36/100\n",
      "479/479 [==============================] - 877s 2s/step - loss: 0.8605 - precision: 0.7677 - recall: 0.4972 - acc: 0.6654 - val_loss: 0.8930 - val_precision: 0.7636 - val_recall: 0.4787 - val_acc: 0.6578 - lr: 2.0000e-05\n",
      "Epoch 37/100\n",
      "479/479 [==============================] - 763s 2s/step - loss: 0.8584 - precision: 0.7679 - recall: 0.4975 - acc: 0.6658 - val_loss: 0.8863 - val_precision: 0.7663 - val_recall: 0.4873 - val_acc: 0.6604 - lr: 4.0000e-06\n",
      "Epoch 38/100\n",
      "479/479 [==============================] - 616s 1s/step - loss: 0.8576 - precision: 0.7694 - recall: 0.4983 - acc: 0.6659 - val_loss: 0.8844 - val_precision: 0.7699 - val_recall: 0.4882 - val_acc: 0.6624 - lr: 4.0000e-06\n",
      "Epoch 39/100\n",
      "479/479 [==============================] - 625s 1s/step - loss: 0.8573 - precision: 0.7678 - recall: 0.5007 - acc: 0.6659 - val_loss: 0.8861 - val_precision: 0.7694 - val_recall: 0.4879 - val_acc: 0.6619 - lr: 4.0000e-06\n",
      "Epoch 40/100\n",
      "248/479 [==============>...............] - ETA: 4:27 - loss: 0.8469 - precision: 0.7741 - recall: 0.5068 - acc: 0.6654"
     ]
    }
   ],
   "source": [
    "# steps_per_epoch: number of batches to be drawn from the generator after assuming epoch over\n",
    "# epochs: number of epochs\n",
    "# validation_steps: how many batches to draw from the validation generator for evaluation\n",
    "\n",
    "import keras\n",
    "import os\n",
    "\n",
    "callbacks_list = [\n",
    "        # interrupts training when accuracy has stopped improving accuracy on the validation set for at least 3+1=4 epochs\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='acc', # should be part of the metrics specific during compilation\n",
    "            patience=5,\n",
    "        ),\n",
    "        # monitor the model's validation loss and reduce the LR when the validation loss has stopped improving, effective strategy to escape local minima\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2, # divides LR by 5 when triggered\n",
    "            patience=3 # called when stopped improving for 3 epochs\n",
    "        )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=int(math.ceil((1. * number_training) / batch_size)),\n",
    "      epochs=100,\n",
    "      class_weight=weights,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=int(math.ceil((1. * number_eval) / batch_size)),\n",
    "      callbacks=callbacks_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "  # 5. Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation ACC')\n",
    "plt.legend()\n",
    "plt.figure()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision = history.history['precision']\n",
    "val_precision = history.history['val_precision']\n",
    "plt.plot(epochs, precision, 'r', label='Training precision')\n",
    "plt.plot(epochs, val_precision, 'b', label='Validation precision')\n",
    "plt.title('Training and validation Precision')\n",
    "plt.legend()\n",
    "plt.figure()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recall = history.history['recall']\n",
    "val_recall = history.history['val_recall']\n",
    "plt.plot(epochs, recall, 'r', label='Training recall')\n",
    "plt.plot(epochs, val_recall, 'b', label='Validation recall')\n",
    "plt.title('Training and validation Recall')\n",
    "plt.legend()\n",
    "plt.figure()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# smooth curves if they look noisy\n",
    "# replace each point with an exponential moving average of the previous points\n",
    "def smooth_curve(points, factor=0.8):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(epochs,\n",
    "         smooth_curve(acc), 'r', label='Smoothed training acc')\n",
    "plt.plot(epochs,\n",
    "         smooth_curve(val_acc), 'b', label='Smoothed validation acc')\n",
    "plt.title('Training and validation MAE')\n",
    "plt.legend()\n",
    "plt.figure()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(epochs,\n",
    "         smooth_curve(loss), 'r', label='Smoothed training loss')\n",
    "plt.plot(epochs,\n",
    "         smooth_curve(val_loss), 'b', label='Smoothed validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# display average, the model may improve even if not reflected"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Early stopping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "\n",
    "callbacks_list = [\n",
    "        # interrupts training when accuracy has stopped improving accuracy on the validation set for at least 3+1=4 epochs\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='acc', # should be part of the metrics specific during compilation\n",
    "            patience=5,\n",
    "        ),\n",
    "        # save the current weights after every epoch\n",
    "        #keras.callbacks.ModelCheckpoint(\n",
    "        #    filepath=os.path.join(paths['MODELS'],'CNN_baseline.h5'),\n",
    "        #    monitor='val_loss', # do not overwrite until val_loss is improved\n",
    "        #    save_best_only=True\n",
    "        #),\n",
    "        # monitor the model's validation loss and reduce the LR when the validation loss has stopped improving, effective strategy to escape local minima\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2, # divides LR by 5 when triggered\n",
    "            patience=3 # called when stopped improving for 3 epochs\n",
    "        ),\n",
    "        #keras.callbacks.TensorBoard(\n",
    "        #    log_dir=paths['LOG_DIR'],\n",
    "        #    write_graph=True,\n",
    "        #    histogram_freq=1 # record activation histograms every 1 epoch\n",
    "        #)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=int(math.ceil((1. * number_training) / batch_size)),\n",
    "      epochs=200,\n",
    "      validation_data=validation_generator,\n",
    "      callbacks=callbacks_list,\n",
    "      validation_steps=int(math.ceil((1. * number_eval) / batch_size)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "  # 7. Model testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check errors test set\n",
    "import os\n",
    "from PIL import Image\n",
    "folder_path = paths['TEST_PATH']\n",
    "extensions = []\n",
    "for fldr in os.listdir(folder_path):\n",
    "    sub_folder_path = os.path.join(folder_path, fldr)\n",
    "    for filee in os.listdir(sub_folder_path):\n",
    "        file_path = os.path.join(sub_folder_path, filee)\n",
    "        print('** Path: {}  **'.format(file_path), end=\"\\r\", flush=True)\n",
    "        im = Image.open(file_path)\n",
    "        rgb_im = im.convert('RGB')\n",
    "        if filee.split('.')[1] not in extensions:\n",
    "            extensions.append(filee.split('.')[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "number_test = 0\n",
    "for i in range(9):\n",
    "      path = os.path.join(paths['TEST_PATH'],str(i))\n",
    "      n_images = len([f for f in os.listdir(path)if os.path.isfile(os.path.join(path, f))])\n",
    "      number_test += n_images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dir=paths['TEST_PATH']\n",
    "print(test_dir)\n",
    "test_datagen = ImageDataGenerator(rescale=rescale_size) # it should not be augmented\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical', classes=None, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "test_generator.reset()\n",
    "Y_pred = model.predict_generator(test_generator, number_test // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = ['0','1','2','3','4','5','6','7','8']\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=target_names))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (12, 8))\n",
    "\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "\n",
    "    for (idx, c_label) in enumerate(target_names):\n",
    "        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
    "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
    "    c_ax.legend()\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "\n",
    "test_generator.reset()\n",
    "y_pred = model.predict_generator(test_generator, verbose = True)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(\"Multiclass roc auc score:\", multiclass_roc_auc_score(test_generator.classes, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Model exportation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save(\"models/CNN_dropout_regularizer_bigger_class_weights.h5\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Plot model as graph of layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, to_file='model.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
