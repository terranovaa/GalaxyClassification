{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# unfreeze a few of the top layers of a frozen model base used for feature extraction and jointly train the new added model and these top layers\n",
    "# the more specialized feature need in fact to be retrained"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.applications import VGG16 # TODO: try also Xception, Inception V3, ResNet50, VGG16, VGG19, MobileNet and others domain-specific nets\n",
    "from keras.applications import Xception\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications import EfficientNetB2\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images_path = os.path.join(\"..\",\"workspace\",\"images\")\n",
    "train_dir = os.path.join(images_path, \"train\")\n",
    "validation_dir = os.path.join(images_path, \"eval\")\n",
    "test_dir = os.path.join(images_path, \"test\")\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# hyperparameters to be set\n",
    "input_height = 256\n",
    "input_width = 256\n",
    "loss_function='mse'\n",
    "metrics=['mae']\n",
    "optimizer='rmsprop'\n",
    "optimizer_learning_rate=1e-4\n",
    "epochs=5\n",
    "num_classes=8\n",
    "batch_size=20\n",
    "if optimizer == 'rmsprop':\n",
    "    optimizer=optimizers.RMSprop(learning_rate=optimizer_learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#import pre-trained models\n",
    "vgg16 = VGG16(weights='imagenet', # weight checkpoint from which we initialize the model\n",
    "                  include_top=False, # to decide if need to include the densely connected layer\n",
    "                  input_shape=(input_width, input_height, 3)) #TODO: check order\n",
    "\n",
    "xception = Xception(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(input_width, input_height, 3))\n",
    "\n",
    "resnet = ResNet50(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(input_width, input_height, 3))\n",
    "\n",
    "efficientnetb2 = EfficientNetB2(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(input_width, input_height, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pretrained_models = [vgg16,xception,resnet,efficientnetb2]\n",
    "names = [\"Vgg16\",\"Xception\",\"Resnet\",\"EfficientNetB2\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#generate train,test and validation set\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest') # set right parameters\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(train_dir, \"df_train.csv\"))\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_model(input_conv_base):\n",
    "    #build the cnn using the pre-trained cnn\n",
    "    built_model = models.Sequential()\n",
    "    built_model.add(input_conv_base)\n",
    "    built_model.add(layers.Flatten())\n",
    "    built_model.add(layers.Dense(256, activation='relu'))\n",
    "    built_model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return built_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model(model_to_evaluate,test_images):\n",
    "\n",
    "    model_loss, model_accuracy = model_to_evaluate.evaluate_generator(test_images, steps=2)\n",
    "\n",
    "    return [model_loss, model_accuracy]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_accuracy_and_loss(history_to_plot):\n",
    "    acc = history_to_plot.history['acc']\n",
    "    val_acc = history_to_plot.history['val_acc']\n",
    "    loss = history_to_plot.history['loss']\n",
    "    val_loss = history_to_plot.history['val_loss']\n",
    "    history_epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(history_epochs, acc, 'r', label='Training acc')\n",
    "    plt.plot(history_epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation MAE')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(history_epochs, loss, 'r', label='Training loss')\n",
    "    plt.plot(history_epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def write_to_csv(metrics_to_write):\n",
    "    header = [\"conv_base_name\",\"acc\",\"val_acc\"]\n",
    "    with open('TRANSFER_LEARNING_RESULTS.csv', 'a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # write header first\n",
    "        writer.writerow(header)\n",
    "\n",
    "        #write all the other metrics\n",
    "        for metric in metrics_to_write:\n",
    "            writer.writerow(metric)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#stores metrics for each fine tuned model\n",
    "model_metrics = []\n",
    "\n",
    "#fine tune and evaluate each model\n",
    "for conv_base_name, conv_base in zip(names,pretrained_models):\n",
    "    conv_base.trainable = True\n",
    "    set_trainable = False\n",
    "    #fine tune\n",
    "    for layer in conv_base.layers:\n",
    "        if layer.name == 'block5_conv1': # TODO: decide the layer where to stop\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "        #build model\n",
    "        model = build_model(conv_base)\n",
    "\n",
    "        model.compile(loss=loss_function,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics)\n",
    "\n",
    "        history = model.fit_generator(\n",
    "              train_generator,\n",
    "              steps_per_epoch=10,\n",
    "              epochs=3,\n",
    "              validation_data=validation_generator,\n",
    "              validation_steps=5)\n",
    "\n",
    "        plot_accuracy_and_loss(history)\n",
    "        cur_metrics = evaluate_model(model,test_generator)\n",
    "        #build list in this format [conv_base_name , model_loss, model_accuracy\n",
    "        model_metrics.append([conv_base_name].extend(cur_metrics))\n",
    "\n",
    "#write metrics to file\n",
    "write_to_csv(model_metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use optimizer with very low learning rate to limit the magnitude of modifications we make"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255) # it should not be augmented\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(input_width, input_height), batch_size=batch_size, class_mode='categorical')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# if performances are much wors than validation ones, during hyperparameter optimization (when done) the process has overfitted the validdation set, if so go to a more clear protocol such as Kfold CV\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=2)\n",
    "print('test acc:', test_acc)\n",
    "print('test loss:', test_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}